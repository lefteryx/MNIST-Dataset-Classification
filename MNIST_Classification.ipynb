{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM5bSalONfyKk96o+71xpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lefteryx/MNIST-Dataset-Classification/blob/main/MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explain the different approaches you took while building this model and the explain why certain approaches failed.*\n",
        "\n",
        "***\n",
        "\n",
        "Overall, building a neural network involves selecting a appropriate functions and algorithms, experimenting with different techniques to improve performance, and carefully monitoring the model's performance on the validation set to prevent overfitting.\n",
        "\n",
        "My approach was pretty much what I had learnt in 3B1B's video on neural networks and in the official PyTorch Documentation-cum-Tutorial: I got a 28*28 unit input layer, a few hidden layers, and a 10 unit output layer. \n",
        "\n",
        "As far as the number of hidden layers are concerned, a single layer would've been too less as it would probably not have enough capacity to learn the complex patterns in the MNIST dataset, resulting in underfit, poor performance.\n",
        "Taking too many layers would've been an issue as well, since it would've increased the complexity of the model making it harder to train, resulting in poor performance in the testing phase due to overfitness.\n",
        "\n",
        "I went with the middle way in choosing the number of neurons in the hidden layers, as well (512 each) for reasons similar to those in the case of the number of hidden layers, as stated above.\n",
        "\n",
        "I preferred ReLu over the Sigmoid function since it's Sigmoid is unnecessarily complex and outdated.\n",
        "\n",
        "I chose an efficient learn rate as well, which if too low would take too much of time, and if too much, may overshoot the minimum of the loss function and fail to converge at all.\n",
        "\n",
        "For the optimization algorithm, I chose Stochastic Gradient Descent (SGD) due to its simplicity and popularity. Upon my research, I found that Adam and RMSProp, among others, may have been more efficient choices, but I couldn't take out time to elaborately study them due to an unforeseen illness. \n",
        "\n",
        "Still, SGD has a lot of benefits, among which 2 of the most important ones I mention below:\n",
        "\n",
        "1. Efficiency - SGD is computationally efficient and can handle even large datasets, converging quickly.\n",
        "\n",
        "2. Simplicity - SGD is a simple algorithm that is easy to implement and understand. It only requires computing the gradient of the loss function with respect to the model's weights, and moving in the opposite direction of the gradient. This makes it a straightforward algorithm to use and debug.\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "cAFMg3xaL45w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Find ways to explain how and why the model converges and at what point overfitting takes place.*\n",
        "***\n",
        "\n",
        "When a neural network is trained, it uses an optimization algorithm to adjust the weights of the network in order to minimize the loss function. This is known as convergence.\n",
        "\n",
        "There are several factors that can influence whether a model will converge and how long it will take to converge. For example, the choice of optimization algorithm, the learning rate, and the size and complexity of the model can all affect convergence.\n",
        "\n",
        "In general, the model converges when the optimization algorithm is able to find the optimal set of weights that minimize the loss function. \n",
        "\n"
      ],
      "metadata": {
        "id": "BpAN2C_dXIVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgNNIdQLHfTv",
        "outputId": "563ab97c-7b19-4ff2-ef5c-5fa6be9ad7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step[938/938], Loss: 0.6509\n",
            "Epoch [2/20], Step[938/938], Loss: 0.4683\n",
            "Epoch [3/20], Step[938/938], Loss: 0.4220\n",
            "Epoch [4/20], Step[938/938], Loss: 0.5973\n",
            "Epoch [5/20], Step[938/938], Loss: 0.2738\n",
            "Epoch [6/20], Step[938/938], Loss: 0.2659\n",
            "Epoch [7/20], Step[938/938], Loss: 0.1819\n",
            "Epoch [8/20], Step[938/938], Loss: 0.6452\n",
            "Epoch [9/20], Step[938/938], Loss: 0.1867\n",
            "Epoch [10/20], Step[938/938], Loss: 0.0864\n",
            "Epoch [11/20], Step[938/938], Loss: 0.0853\n",
            "Epoch [12/20], Step[938/938], Loss: 0.1220\n",
            "Epoch [13/20], Step[938/938], Loss: 0.0432\n",
            "Epoch [14/20], Step[938/938], Loss: 0.0695\n",
            "Epoch [15/20], Step[938/938], Loss: 0.0793\n",
            "Epoch [16/20], Step[938/938], Loss: 0.1964\n",
            "Epoch [17/20], Step[938/938], Loss: 0.1369\n",
            "Epoch [18/20], Step[938/938], Loss: 0.0918\n",
            "Epoch [19/20], Step[938/938], Loss: 0.0625\n",
            "Epoch [20/20], Step[938/938], Loss: 0.0825\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 96.16%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define the layers of the network\n",
        "        self.fc1 = nn.Linear(28 * 28, 512) # Fully-connected layer with 512 units\n",
        "        self.fc2 = nn.Linear(512, 512) # Fully-connected layer with 512 units\n",
        "        self.fc3 = nn.Linear(512, 10) # Fully-connected layer with 10 units\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass through the network\n",
        "        x = x.view(-1, 28 * 28) # Reshape the input tensor into a 2D tensor\n",
        "        x = F.relu(self.fc1(x)) # Apply ReLU activation function to the output of the first fully-connected layer\n",
        "        x = F.relu(self.fc2(x)) # Apply ReLU activation function to the output of the second fully-connected layer\n",
        "        x = self.fc3(x) # Pass the output of the second fully-connected layer through the third fully-connected layer\n",
        "        return x\n",
        "\n",
        "# Set hyperparameters: very important for the performance of our model\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='data', train=True, download=True,\n",
        "                               transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='data', train=False, download=True,\n",
        "                              transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Instantiating NeuralNetwork and creating an optimizer object\n",
        "model = NeuralNetwork()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the network\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Pass the data through the network\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx+1==938:\n",
        "             print (f'Epoch [{epoch+1}/{num_epochs}], Step[{batch_idx+1}/{n_total_steps}], Loss: {loss.item():.4f}') \n",
        "\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28)\n",
        "        labels = labels\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item() \n",
        "        \n",
        "    # accuracy of the Neural Network \n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print()\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc}%') \n",
        "\n"
      ]
    }
  ]
}